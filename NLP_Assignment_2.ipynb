{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiyuanliang666/NLP/blob/main/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM8rcgktVoHV"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "-h5JlZQn9H99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463d153f-0143-4a02-a42c-4f557bb30899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmKXEj7rMI9E"
      },
      "source": [
        "Translation with a Sequence to Sequence Network\n",
        "*************************************************************\n",
        "\n",
        "::\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "... to varying degrees of success.\n",
        "\n",
        "This is made possible by the simple but powerful idea of the `sequence\n",
        "to sequence network <http://arxiv.org/abs/1409.3215>`__, in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "To improve upon this model we'll use an `attention\n",
        "mechanism <https://arxiv.org/abs/1409.0473>`__, which lets the decoder\n",
        "learn to focus over a specific range of the input sequence.\n",
        "\n",
        "**Recommended Reading:**\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and\n",
        "understand Tensors:\n",
        "\n",
        "-  http://pytorch.org/ For installation instructions\n",
        "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
        "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
        "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
        "\n",
        "\n",
        "It would also be useful to know about Sequence to Sequence networks and\n",
        "how they work:\n",
        "\n",
        "-  `Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "   Statistical Machine Translation <http://arxiv.org/abs/1406.1078>`__\n",
        "-  `Sequence to Sequence Learning with Neural\n",
        "   Networks <http://arxiv.org/abs/1409.3215>`__\n",
        "-  `Neural Machine Translation by Jointly Learning to Align and\n",
        "   Translate <https://arxiv.org/abs/1409.0473>`__\n",
        "-  `A Neural Conversational Model <http://arxiv.org/abs/1506.05869>`__\n",
        "\n",
        "You will also find the previous tutorials on\n",
        ":doc:`/intermediate/char_rnn_classification_tutorial`\n",
        "and :doc:`/intermediate/char_rnn_generation_tutorial`\n",
        "helpful as those concepts are very similar to the Encoder and Decoder\n",
        "models, respectively.\n",
        "\n",
        "And for more, read the papers that introduced these topics:\n",
        "\n",
        "-  `Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "   Statistical Machine Translation <http://arxiv.org/abs/1406.1078>`__\n",
        "-  `Sequence to Sequence Learning with Neural\n",
        "   Networks <http://arxiv.org/abs/1409.3215>`__\n",
        "-  `Neural Machine Translation by Jointly Learning to Align and\n",
        "   Translate <https://arxiv.org/abs/1409.0473>`__\n",
        "-  `A Neural Conversational Model <http://arxiv.org/abs/1506.05869>`__\n",
        "\n",
        "\n",
        "**Requirements**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrgMvmAeMI9J"
      },
      "source": [
        "Loading data files\n",
        "==================\n",
        "\n",
        "The data for this project is a set of many thousands of English to\n",
        "French translation pairs.\n",
        "\n",
        "`This question on Open Data Stack\n",
        "Exchange <http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>`__\n",
        "pointed me to the open translation site http://tatoeba.org/ which has\n",
        "downloads available at http://tatoeba.org/eng/downloads - and better\n",
        "yet, someone did the extra work of splitting language pairs into\n",
        "individual text files here: http://www.manythings.org/anki/\n",
        "\n",
        "The English to French pairs are too big to include in the repo, so\n",
        "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
        "separated list of translation pairs:\n",
        "\n",
        "::\n",
        "\n",
        "    I am cold.    J'ai froid.\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip -o fra-eng.zip\n",
        "!mkdir data\n",
        "!mv fra.txt data/eng-fra.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUGpVT3MC9ZA",
        "outputId": "da45802e-920c-4102-a934-612f2fd287f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-09 02:31:28--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8186368 (7.8M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.81M  36.3MB/s    in 0.2s    \n",
            "\n",
            "2025-11-09 02:31:28 (36.3 MB/s) - ‘fra-eng.zip’ saved [8186368/8186368]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang: # data preprocessing\n",
        "    def __init__(self, name):\n",
        "        self.name = name # language name \"English\" / \"French\"\n",
        "        self.word2index = {} # eg. \"hello\" -> 3\n",
        "        self.word2count = {} # eg. \"hello\" -> 5\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"} # initialization: start and end marks\n",
        "        self.n_words = 2  # Count SOS and EOS, the size of the current vocabulary list\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "RZKd8K4BfBM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQpwb8giMI9S"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s) # transfer Unicode string s to NFD form\n",
        "        if unicodedata.category(c) != 'Mn' # remove diacritics and other non-ASCII characters\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "WtFJGPPytPV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49xLMyaYMI9X"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the ``reverse``\n",
        "flag to reverse the pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(lang1, lang2, reverse=False): # original， tartget，if reverse\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n') # Remove leading and trailing whitespace characters and split by line.\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines] # Each row is split by a tab character, containing multiple fields; the first two fields are selected.\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "AVZyk7glkhZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QGX-Ffq6Iii"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences. Here the maximum length is 10 words (that includes\n",
        "ending punctuation) and we're filtering to sentences that translate to\n",
        "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "earlier).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 15\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am\", \"i m\",\n",
        "    \"he is\", \"he s\",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re\",\n",
        "    \"we are\", \"we re\",\n",
        "    \"they are\", \"they re\"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "8t1N5cYalfJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A4Kf_ie6Jyr"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "#print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnGQl472lg0-",
        "outputId": "b5bda8ce-55d7-4055-a70e-a0f43dea1407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 239189 sentence pairs\n",
            "Trimmed to 23543 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 7142\n",
            "eng 4737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Tt6luSw3ym6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = [i[0] for i in pairs]\n",
        "y = [i[1] for i in pairs]"
      ],
      "metadata": {
        "id": "R50-9amYytmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "Ej3a3AvYyoI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs = list(zip(X_train,y_train))\n",
        "test_pairs = list(zip(X_test,y_test))"
      ],
      "metadata": {
        "id": "X-69f4nmy5ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XxL347dMI9p"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
        "seq2seq network, or `Encoder Decoder\n",
        "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "black cat\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YkUkdswMI9q"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "O0aZQW30liTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvouslB-MI9t"
      },
      "source": [
        "The Decoder (Your assignment)\n",
        "-----------\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccvXiDVAMI9u"
      },
      "source": [
        "Simple Decoder\n",
        "^^^^^^^^^^^^^^\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        # Your code here #\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "DNY63vmP7pZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJTli8NHMI91"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
        "  limitation by using a relative position approach. Read about \"local\n",
        "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
        "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
        "\n",
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "WrWHiE1RLtCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-s0fPrIMI99"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but `when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "CvKmiwlnLtFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxwlsYBxMI-A"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "4Wb3PO24LwhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu_oDmPEMI-D"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, epochs, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    iter = 1\n",
        "    n_iters = len(train_pairs) * epochs\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch: %d/%d\" % (epoch, epochs))\n",
        "        for training_pair in train_pairs:\n",
        "            training_pair = tensorsFromPair(training_pair)\n",
        "\n",
        "            input_tensor = training_pair[0]\n",
        "            target_tensor = training_pair[1]\n",
        "\n",
        "            loss = train(input_tensor, target_tensor, encoder,\n",
        "                        decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "            print_loss_total += loss\n",
        "            plot_loss_total += loss\n",
        "\n",
        "            if iter % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                            iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "            iter +=1"
      ],
      "metadata": {
        "id": "7mPu4xWzLx-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s52RCR0MI-K"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "metadata": {
        "id": "tCrTcoH0L04g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DXbiFX1MI-N"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "XoYOvTaOL4oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def inference(encoder, decoder, testing_pairs):\n",
        "    input = []\n",
        "    gt = []\n",
        "    predict = []\n",
        "\n",
        "    from tqdm import tqdm\n",
        "    for i in tqdm(range(len(testing_pairs))):\n",
        "        pair = testing_pairs[i]\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "\n",
        "        input.append(pair[0])\n",
        "        gt.append(pair[1])\n",
        "        predict.append(output_sentence)\n",
        "\n",
        "    return input,gt,predict\n",
        "\n",
        "\n",
        "def eval(gt, predict):\n",
        "  rouge = ROUGEScore()\n",
        "  metric_score = rouge(predict, gt)\n",
        "  print(\"=== Evaluation score - Rouge score ===\")\n",
        "  print(\"Rouge1 fmeasure:\\t\",metric_score[\"rouge1_fmeasure\"].item())\n",
        "  print(\"Rouge1 precision:\\t\",metric_score[\"rouge1_precision\"].item())\n",
        "  print(\"Rouge1 recall:  \\t\",metric_score[\"rouge1_recall\"].item())\n",
        "  print(\"Rouge2 fmeasure:\\t\",metric_score[\"rouge2_fmeasure\"].item())\n",
        "  print(\"Rouge2 precision:\\t\",metric_score[\"rouge2_precision\"].item())\n",
        "  print(\"Rouge2 recall:  \\t\",metric_score[\"rouge2_recall\"].item())\n",
        "  print(\"=====================================\")"
      ],
      "metadata": {
        "id": "1DWJXsfd2q4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppy3qh7fMI-R"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = Decoder(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 5, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzMeJbVDL6Un",
        "outputId": "17a20f27-bd31-43bb-f258-1276f42fe02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/5\n",
            "1m 15s (- 25m 15s) (5000 4%) 3.3327\n",
            "2m 31s (- 24m 14s) (10000 9%) 2.8679\n",
            "3m 48s (- 23m 3s) (15000 14%) 2.6160\n",
            "5m 5s (- 21m 54s) (20000 18%) 2.4385\n",
            "Epoch: 1/5\n",
            "6m 23s (- 20m 41s) (25000 23%) 2.2555\n",
            "7m 41s (- 19m 27s) (30000 28%) 2.0856\n",
            "8m 58s (- 18m 11s) (35000 33%) 1.9965\n",
            "10m 16s (- 16m 56s) (40000 37%) 1.8762\n",
            "Epoch: 2/5\n",
            "11m 34s (- 15m 40s) (45000 42%) 1.8011\n",
            "12m 51s (- 14m 23s) (50000 47%) 1.6746\n",
            "14m 9s (- 13m 6s) (55000 51%) 1.6049\n",
            "15m 27s (- 11m 50s) (60000 56%) 1.5358\n",
            "Epoch: 3/5\n",
            "16m 45s (- 10m 33s) (65000 61%) 1.4951\n",
            "18m 3s (- 9m 16s) (70000 66%) 1.3860\n",
            "19m 20s (- 7m 58s) (75000 70%) 1.3431\n",
            "20m 38s (- 6m 41s) (80000 75%) 1.2745\n",
            "Epoch: 4/5\n",
            "21m 56s (- 5m 24s) (85000 80%) 1.2678\n",
            "23m 15s (- 4m 7s) (90000 84%) 1.1942\n",
            "24m 32s (- 2m 49s) (95000 89%) 1.1270\n",
            "25m 50s (- 1m 32s) (100000 94%) 1.0964\n",
            "27m 8s (- 0m 14s) (105000 99%) 1.0828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "metadata": {
        "id": "gc2y_EGQUT-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1d0f7f-a1c4-4d58-f319-270f5e256a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> il se gratta la tete .\n",
            "= he scratched his head .\n",
            "< he shook his head . <EOS>\n",
            "\n",
            "> je ne suis pas habituee a faire ca dans l obscurite .\n",
            "= i m not used to doing that in the dark .\n",
            "< i m not accustomed to doing this . <EOS>\n",
            "\n",
            "> je l ai prise pour sa s ur . elles se ressemblent tellement .\n",
            "= i mistook her for her sister . they look so much alike .\n",
            "< i mistook her friends so i can help her . <EOS>\n",
            "\n",
            "> il est celui qui a le plus de chances de reussir .\n",
            "= he s the most likely to succeed .\n",
            "< he is the likely to to succeed . <EOS>\n",
            "\n",
            "> il est a l eglise en ce moment .\n",
            "= he s at church right now .\n",
            "< he is now up now . <EOS>\n",
            "\n",
            "> il me faut y aller seul .\n",
            "= i must go alone .\n",
            "< i must go alone . <EOS>\n",
            "\n",
            "> je suis fauche .\n",
            "= i m broke .\n",
            "< i m very . <EOS>\n",
            "\n",
            "> vous n etes pas censes faire cela n est ce pas ?\n",
            "= you aren t supposed to do that are you ?\n",
            "< you aren t supposed to do that are you ? <EOS>\n",
            "\n",
            "> je dois partir quelque part .\n",
            "= i must go somewhere .\n",
            "< i must leave my . <EOS>\n",
            "\n",
            "> tu es talentueuse .\n",
            "= you re talented .\n",
            "< you re talented . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input,gt,predict = inference(encoder1, decoder1, train_pairs)\n",
        "# eval(gt, predict)"
      ],
      "metadata": {
        "id": "mcJQDHJ2-96c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input,gt,predict = inference(encoder1, decoder1, test_pairs)\n",
        "eval(gt, predict)"
      ],
      "metadata": {
        "id": "F81S9tbV6vtL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e95bd5-751c-4035-a58d-b0bea8a4dbc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2355/2355 [00:12<00:00, 191.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.6257568597793579\n",
            "Rouge1 precision:\t 0.5845078229904175\n",
            "Rouge1 recall:  \t 0.681381106376648\n",
            "Rouge2 fmeasure:\t 0.4463028013706207\n",
            "Rouge2 precision:\t 0.4093894064426422\n",
            "Rouge2 recall:  \t 0.4985301196575165\n",
            "=====================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 2: Replace GRU with LSTM in Encoder & Decoder"
      ],
      "metadata": {
        "id": "4rDuvj0yNAi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# ---------------- EncoderLSTM ----------------\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        # h_0, c_0 initialize 0\n",
        "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
        "                torch.zeros(1, 1, self.hidden_size, device=device))\n",
        "\n",
        "# ---------------- DecoderLSTM ----------------\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # hidden-(h, c)\n",
        "        h, c = hidden\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, (h, c) = self.lstm(output, (h, c))\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, (h, c)\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
        "                torch.zeros(1, 1, self.hidden_size, device=device))\n"
      ],
      "metadata": {
        "id": "WuacX-OHHNnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder_lstm = EncoderLSTM(input_lang.n_words, hidden_size).to(device)\n",
        "decoder_lstm = DecoderLSTM(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder_lstm, decoder_lstm, epochs=5, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhtSAKYGNMse",
        "outputId": "93d44367-9281-4b4a-be3d-6e6769715a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/5\n",
            "1m 35s (- 32m 11s) (5000 4%) 3.4031\n",
            "3m 1s (- 29m 4s) (10000 9%) 3.0374\n",
            "4m 26s (- 26m 57s) (15000 14%) 2.8388\n",
            "5m 51s (- 25m 12s) (20000 18%) 2.6760\n",
            "Epoch: 1/5\n",
            "7m 17s (- 23m 35s) (25000 23%) 2.4950\n",
            "8m 51s (- 22m 25s) (30000 28%) 2.3372\n",
            "10m 27s (- 21m 10s) (35000 33%) 2.2443\n",
            "11m 53s (- 19m 36s) (40000 37%) 2.1427\n",
            "Epoch: 2/5\n",
            "13m 19s (- 18m 2s) (45000 42%) 2.0506\n",
            "14m 44s (- 16m 29s) (50000 47%) 1.9342\n",
            "16m 9s (- 14m 58s) (55000 51%) 1.8778\n",
            "17m 35s (- 13m 28s) (60000 56%) 1.7979\n",
            "Epoch: 3/5\n",
            "19m 1s (- 11m 58s) (65000 61%) 1.7612\n",
            "20m 26s (- 10m 29s) (70000 66%) 1.6671\n",
            "21m 51s (- 9m 1s) (75000 70%) 1.5957\n",
            "23m 17s (- 7m 33s) (80000 75%) 1.5432\n",
            "Epoch: 4/5\n",
            "24m 43s (- 6m 5s) (85000 80%) 1.5248\n",
            "26m 8s (- 4m 37s) (90000 84%) 1.4393\n",
            "27m 34s (- 3m 10s) (95000 89%) 1.3863\n",
            "28m 59s (- 1m 43s) (100000 94%) 1.3519\n",
            "30m 25s (- 0m 16s) (105000 99%) 1.3267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate Randomly on Training Pairs ===\")\n",
        "evaluateRandomly(encoder_lstm, decoder_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKpeimymNNGQ",
        "outputId": "cf4bb03e-1d32-4ecc-954a-10c812dbdea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate Randomly on Training Pairs ===\n",
            "> ils font de l obstruction pour empecher le projet de loi de passer .\n",
            "= they are filibustering to prevent the bill from passing .\n",
            "< they re looking for a the of the the . . . <EOS>\n",
            "\n",
            "> c est moi qui en suis responsable .\n",
            "= i am to blame for it .\n",
            "< i m about to that . <EOS>\n",
            "\n",
            "> nous sortons pour celebrer .\n",
            "= we re going out to celebrate .\n",
            "< we re getting for a . . <EOS>\n",
            "\n",
            "> je suis un peu au milieu de quelque chose .\n",
            "= i m kind of in the middle of something .\n",
            "< i m sort of the something of something . <EOS>\n",
            "\n",
            "> il est americain .\n",
            "= he is an american .\n",
            "< he is poor . <EOS>\n",
            "\n",
            "> ils sont tous semblables .\n",
            "= they re all alike .\n",
            "< they re all alike . <EOS>\n",
            "\n",
            "> je crains que vous n ayez le mauvais numero .\n",
            "= i m afraid that you have the wrong number .\n",
            "< i m afraid you you the wrong number . <EOS>\n",
            "\n",
            "> j ai hate d aller au zoo avec vous .\n",
            "= i m looking forward to going to the zoo with you .\n",
            "< i m looking forward to going to to party you . <EOS>\n",
            "\n",
            "> il est habitue a parler en public .\n",
            "= he is accustomed to speaking in public .\n",
            "< he is used to speaking in public . <EOS>\n",
            "\n",
            "> je ne suis pas totalement convaincu que vous ayez raison .\n",
            "= i m not wholly convinced you re right .\n",
            "< i m not convinced that you re right . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate on Test Set (Rouge) ===\")\n",
        "input, gt, predict = inference(encoder_lstm, decoder_lstm, test_pairs)\n",
        "eval(gt, predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcWaBxAJNNL8",
        "outputId": "d1fa6e21-4eca-42f8-9001-f7d3865b9747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate on Test Set (Rouge) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2355/2355 [00:12<00:00, 181.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.6129138469696045\n",
            "Rouge1 precision:\t 0.5790438055992126\n",
            "Rouge1 recall:  \t 0.6589285731315613\n",
            "Rouge2 fmeasure:\t 0.4317256808280945\n",
            "Rouge2 precision:\t 0.400726318359375\n",
            "Rouge2 recall:  \t 0.47588860988616943\n",
            "=====================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 3: Bi-LSTM Encoder + GRU Decoder"
      ],
      "metadata": {
        "id": "ugXEVtR4OEa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# ---------------- BiLSTM Encoder ----------------\n",
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderBiLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # Bi LSTM\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n",
        "        # Compress the bidirectional output from 2*hidden_size back to hidden_size, and use it as input to the Decoder.\n",
        "        self.fc_out = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc_h = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc_c = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden_tuple):\n",
        "        if isinstance(hidden_tuple, tuple):\n",
        "            hidden, cell = hidden_tuple\n",
        "        else:\n",
        "            # Pass None on the first call\n",
        "            hidden = torch.zeros(2, 1, self.hidden_size, device=device)\n",
        "            cell = torch.zeros(2, 1, self.hidden_size, device=device)\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, (h, c) = self.lstm(embedded, (hidden, cell))\n",
        "        # splicing the last hidden state of the forward and reverse directions\n",
        "        h_cat = torch.cat((h[-2,:,:], h[-1,:,:]), dim=1)  # (1, hidden*2)\n",
        "        c_cat = torch.cat((c[-2,:,:], c[-1,:,:]), dim=1)\n",
        "        # Compressed to a unidirectional hidden_size\n",
        "        h_out = torch.tanh(self.fc_h(h_cat)).unsqueeze(0)\n",
        "        c_out = torch.tanh(self.fc_c(c_cat)).unsqueeze(0)\n",
        "        output = self.fc_out(output.squeeze(0)).unsqueeze(0)\n",
        "        return output, h_out\n",
        "\n",
        "    def initHidden(self):\n",
        "        # Bi LSTM → num_layers*2\n",
        "        return (torch.zeros(2, 1, self.hidden_size, device=device),\n",
        "                torch.zeros(2, 1, self.hidden_size, device=device))\n",
        "\n",
        "# ---------------- Decoder (same as baseline GRU) ----------------\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderGRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "MVXpsrWKOE_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder_bilstm = EncoderBiLSTM(input_lang.n_words, hidden_size).to(device)\n",
        "decoder_gru = DecoderGRU(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder_bilstm, decoder_gru, epochs=5, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fuc3LLwlOPVS",
        "outputId": "dcf4cdaa-bbbc-4e02-b030-2b7973a7a4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/5\n",
            "1m 20s (- 27m 15s) (5000 4%) 3.4774\n",
            "2m 41s (- 25m 49s) (10000 9%) 3.2425\n",
            "4m 2s (- 24m 29s) (15000 14%) 3.2080\n",
            "5m 22s (- 23m 7s) (20000 18%) 3.1303\n",
            "Epoch: 1/5\n",
            "6m 43s (- 21m 47s) (25000 23%) 3.0782\n",
            "8m 5s (- 20m 27s) (30000 28%) 3.0512\n",
            "9m 25s (- 19m 6s) (35000 33%) 3.0567\n",
            "10m 45s (- 17m 43s) (40000 37%) 2.9774\n",
            "Epoch: 2/5\n",
            "12m 5s (- 16m 22s) (45000 42%) 2.9376\n",
            "13m 29s (- 15m 5s) (50000 47%) 2.9359\n",
            "14m 50s (- 13m 45s) (55000 51%) 2.9347\n",
            "16m 11s (- 12m 23s) (60000 56%) 2.9108\n",
            "Epoch: 3/5\n",
            "17m 40s (- 11m 7s) (65000 61%) 2.8914\n",
            "19m 8s (- 9m 49s) (70000 66%) 2.8704\n",
            "20m 34s (- 8m 29s) (75000 70%) 2.8507\n",
            "22m 3s (- 7m 9s) (80000 75%) 2.8333\n",
            "Epoch: 4/5\n",
            "23m 31s (- 5m 47s) (85000 80%) 2.8369\n",
            "24m 57s (- 4m 25s) (90000 84%) 2.8094\n",
            "26m 23s (- 3m 2s) (95000 89%) 2.7959\n",
            "27m 43s (- 1m 38s) (100000 94%) 2.7794\n",
            "29m 4s (- 0m 15s) (105000 99%) 2.7636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate Randomly on Training Pairs ===\")\n",
        "evaluateRandomly(encoder_bilstm, decoder_gru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glBADOObOPZu",
        "outputId": "2075c5f3-2387-4921-9757-44606ed65be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate Randomly on Training Pairs ===\n",
            "> je suis desole de vous deranger .\n",
            "= i m sorry to bother you .\n",
            "< i m not to . . <EOS>\n",
            "\n",
            "> tu es un petit morveux .\n",
            "= you re a snotty little brat .\n",
            "< i m not to . . <EOS>\n",
            "\n",
            "> vous etes fort ingenieuses .\n",
            "= you re very clever .\n",
            "< i m not to . . <EOS>\n",
            "\n",
            "> je ne vais pas me disputer avec vous !\n",
            "= i m not going to argue with you .\n",
            "< i m not to . . <EOS>\n",
            "\n",
            "> tu es naive .\n",
            "= you re naive .\n",
            "< i m not to . . <EOS>\n",
            "\n",
            "> vous etes aventureuse .\n",
            "= you re adventurous .\n",
            "< i m not to . . <EOS>\n",
            "\n",
            "> vous etes hors de danger .\n",
            "= you re out of danger .\n",
            "< i m not to . . <EOS>\n",
            "\n",
            "> je suis sur qu il reussira l examen .\n",
            "= i am certain that he will pass the exam .\n",
            "< i m not to . . <EOS>\n",
            "\n",
            "> je suis mariee a une avocate .\n",
            "= i m married to a lawyer .\n",
            "< i m not to . . <EOS>\n",
            "\n",
            "> je ne vais pas aller a l ecole aujourd hui .\n",
            "= i m not going to school today .\n",
            "< i m not to . . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate on Test Set (Rouge) ===\")\n",
        "input, gt, predict = inference(encoder_bilstm, decoder_gru, test_pairs)\n",
        "eval(gt, predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1LDamlXOPeH",
        "outputId": "77e188db-3a22-47ee-a61f-13887f80286c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate on Test Set (Rouge) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2355/2355 [00:18<00:00, 130.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.22234992682933807\n",
            "Rouge1 precision:\t 0.2541826069355011\n",
            "Rouge1 recall:  \t 0.20713137090206146\n",
            "Rouge2 fmeasure:\t 0.10987351834774017\n",
            "Rouge2 precision:\t 0.12590233981609344\n",
            "Rouge2 recall:  \t 0.10480139404535294\n",
            "=====================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "修正版本bilstm-encoder"
      ],
      "metadata": {
        "id": "PBWY4tGQ6q0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from torchmetrics.functional.text.rouge import rouge_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MAX_LENGTH = 15\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# ---------------- BiLSTM Encoder (match GRU Decoder) ----------------\n",
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderBiLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=1, bidirectional=True)\n",
        "        self.fc_h = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden_tuple):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, (h_out, c_out) = self.lstm(embedded, hidden_tuple)\n",
        "        return output, (h_out, c_out)\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(2, 1, self.hidden_size, device=device),\n",
        "                torch.zeros(2, 1, self.hidden_size, device=device))\n",
        "\n",
        "    def get_decoder_hidden(self, h_final):\n",
        "        # h_final: [2, 1, hidden_size]\n",
        "        h_cat = torch.cat((h_final[-2,:,:], h_final[-1,:,:]), dim=1)  # [1, 2 * hidden_size]\n",
        "        decoder_hidden = torch.tanh(self.fc_h(h_cat)).unsqueeze(0)\n",
        "        return decoder_hidden\n",
        "\n",
        "# ---------------- Decoder GRU ----------------\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderGRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# ---------------- train function (match Bi-LSTM) ----------------\n",
        "def train(input_tensor, target_tensor, encoder, decoder,\n",
        "          encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "\n",
        "    # EncoderBiLSTM return (h, c) tuple\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # GRU Decoder don't need encoder_outputs. Therefore, the encoder_outputs tensor does not need to be defined here.\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # 1. Encoder loop：Correctly transmits (h, c) tuples\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        # encoder_hidden here is the updated (h, c) tuple.\n",
        "\n",
        "    # 2. State compression: Transforming the final state of the Bi-LSTM into the initial state of the GRU.\n",
        "    # encoder_hidden is (h_final, c_final)\n",
        "    h_final = encoder_hidden[0] # get h state [2, 1, hidden_size]\n",
        "    decoder_hidden = encoder.get_decoder_hidden(h_final) # use method in Encoder to compress and transfer\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "    else:\n",
        "        # Without teacher forcing: use its own prediction as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # Detach from history as input\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / target_length\n",
        "\n",
        "# ---------------- evaluate function (match Bi-LSTM) ----------------\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Evaluates the Seq2Seq model on a single sentence.\n",
        "    This version is fixed to handle the (h, c) tuple of the Bi-LSTM Encoder\n",
        "    and the tuple returned by tensorsFromPair.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        input_tensor, _ = tensorsFromPair([sentence, ''])\n",
        "        input_length = input_tensor.size()[0]\n",
        "\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "\n",
        "        h_final = encoder_hidden[0]\n",
        "        decoder_hidden = encoder.get_decoder_hidden(h_final)\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "        decoded_words = []\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "metadata": {
        "id": "4r6K5jr36xnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder_bilstm = EncoderBiLSTM(input_lang.n_words, hidden_size).to(device)\n",
        "decoder_gru = DecoderGRU(hidden_size, output_lang.n_words).to(device)\n",
        "trainIters(encoder_bilstm, decoder_gru, epochs=5, print_every=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k7b2aq58jGH",
        "outputId": "37f633e0-8ead-4028-bbe3-9e6ec0033edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/5\n",
            "1m 39s (- 33m 24s) (5000 4%) 3.4484\n",
            "3m 18s (- 31m 42s) (10000 9%) 3.0274\n",
            "4m 58s (- 30m 6s) (15000 14%) 2.8729\n",
            "6m 39s (- 28m 36s) (20000 18%) 2.6978\n",
            "Epoch: 1/5\n",
            "8m 21s (- 27m 4s) (25000 23%) 2.4994\n",
            "10m 3s (- 25m 26s) (30000 28%) 2.3265\n",
            "11m 45s (- 23m 50s) (35000 33%) 2.2179\n",
            "13m 27s (- 22m 11s) (40000 37%) 2.1039\n",
            "Epoch: 2/5\n",
            "15m 10s (- 20m 33s) (45000 42%) 1.9947\n",
            "16m 53s (- 18m 53s) (50000 47%) 1.8672\n",
            "18m 35s (- 17m 13s) (55000 51%) 1.7846\n",
            "20m 17s (- 15m 32s) (60000 56%) 1.7081\n",
            "Epoch: 3/5\n",
            "22m 0s (- 13m 51s) (65000 61%) 1.6507\n",
            "23m 42s (- 12m 10s) (70000 66%) 1.5378\n",
            "25m 24s (- 10m 28s) (75000 70%) 1.4950\n",
            "27m 7s (- 8m 47s) (80000 75%) 1.4155\n",
            "Epoch: 4/5\n",
            "28m 50s (- 7m 6s) (85000 80%) 1.4132\n",
            "30m 33s (- 5m 24s) (90000 84%) 1.3226\n",
            "32m 16s (- 3m 42s) (95000 89%) 1.2513\n",
            "33m 57s (- 2m 1s) (100000 94%) 1.2288\n",
            "35m 40s (- 0m 19s) (105000 99%) 1.1969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n=== Evaluate Randomly on Training Pairs ===\")\n",
        "evaluateRandomly(encoder_bilstm, decoder_gru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK3hSxYv9Dfa",
        "outputId": "f839a33d-3a0c-4932-edc5-563a2f5da99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate Randomly on Training Pairs ===\n",
            "> vous etes pleine de surprises .\n",
            "= you re full of surprises .\n",
            "< you re so talented . <EOS>\n",
            "\n",
            "> je vous ai peut etre deja raconte cette histoire .\n",
            "= i may have told you this story before .\n",
            "< i may have you before i ve made you . <EOS>\n",
            "\n",
            "> je suis heureuse pour vous deux .\n",
            "= i m happy for you both .\n",
            "< i m happy for you . <EOS>\n",
            "\n",
            "> elle parle chinois .\n",
            "= she speaks chinese .\n",
            "< she speaks chinese . <EOS>\n",
            "\n",
            "> on dirait qu il est riche .\n",
            "= he seems to be rich .\n",
            "< he seems to be rich now . <EOS>\n",
            "\n",
            "> j essaie de proteger tom .\n",
            "= i m trying to protect tom .\n",
            "< i m trying to protect tom . <EOS>\n",
            "\n",
            "> je ne suis pas du tout convaincu .\n",
            "= i m not convinced at all .\n",
            "< i m not a at all <EOS>\n",
            "\n",
            "> il ne m a jamais frappe auparavant .\n",
            "= he s never hit me before .\n",
            "< he sent me before before . <EOS>\n",
            "\n",
            "> il est populaire aupres de tout le monde .\n",
            "= he is popular with everybody .\n",
            "< he is popular with everybody . <EOS>\n",
            "\n",
            "> vous etes la femme la plus belle au monde .\n",
            "= you re the most beautiful woman in the whole world .\n",
            "< you re the most beautiful in in the world . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate on Test Set (Rouge) ===\")\n",
        "input, gt, predict = inference(encoder_bilstm, decoder_gru, test_pairs)\n",
        "eval(gt, predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEYR0zYi9DkE",
        "outputId": "59d26ed2-853e-4a5f-e712-e87bbb704b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate on Test Set (Rouge) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2355/2355 [00:13<00:00, 172.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.6115792393684387\n",
            "Rouge1 precision:\t 0.5845984220504761\n",
            "Rouge1 recall:  \t 0.6505200862884521\n",
            "Rouge2 fmeasure:\t 0.42236021161079407\n",
            "Rouge2 precision:\t 0.3969039022922516\n",
            "Rouge2 recall:  \t 0.46048763394355774\n",
            "=====================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 4: Add Attention between Encoder and Decoder (Bahdanau)"
      ],
      "metadata": {
        "id": "3UdiY-ArPz0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MAX_LENGTH = 15\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# ---- Attention ----\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        max_len = encoder_outputs.size(0)\n",
        "        hidden = hidden.repeat(max_len, 1, 1).transpose(0, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs.unsqueeze(0)), 2)))\n",
        "        energy = energy.squeeze(0)\n",
        "        energy = torch.matmul(energy, self.v)\n",
        "        attn_weights = F.softmax(energy, dim=0)\n",
        "        return attn_weights.unsqueeze(0)\n",
        "\n",
        "# ---- Encoder ----\n",
        "class EncoderRNN_Attn(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN_Attn, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# ---- Decoder with Attention（include buffer, only set once） ----\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self._encoder_outputs_buffer = None\n",
        "\n",
        "    def set_encoder_outputs(self, encoder_outputs):\n",
        "        self._encoder_outputs_buffer = encoder_outputs\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        encoder_outputs = self._encoder_outputs_buffer\n",
        "        if encoder_outputs is None:\n",
        "            raise RuntimeError(\"Must call set_encoder_outputs before decoding.\")\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        attn_weights = self.attention(hidden, encoder_outputs)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.gru(rnn_input, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# ---- train ----\n",
        "def train_with_attention(input_tensor, target_tensor, encoder, decoder,\n",
        "                         encoder_optimizer, decoder_optimizer, criterion,\n",
        "                         max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "    decoder.set_encoder_outputs(encoder_outputs)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    use_teacher_forcing = True if torch.rand(1).item() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / target_length\n",
        "\n",
        "def trainIters_with_attention(encoder, decoder, epochs, print_every=1000, learning_rate=0.01):\n",
        "    import time, math\n",
        "    def asMinutes(s):\n",
        "        m = math.floor(s / 60)\n",
        "        s -= m * 60\n",
        "        return '%dm %ds' % (m, s)\n",
        "    def timeSince(since, percent):\n",
        "        now = time.time()\n",
        "        s = now - since\n",
        "        es = s / (percent)\n",
        "        rs = es - s\n",
        "        return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "    start = time.time()\n",
        "    print_loss_total = 0\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "    n_iters = len(train_pairs) * epochs\n",
        "    it = 1\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch: {epoch+1}/{epochs}\")\n",
        "        for training_pair in train_pairs:\n",
        "            input_tensor, target_tensor = tensorsFromPair(training_pair)\n",
        "            loss = train_with_attention(input_tensor, target_tensor, encoder, decoder,\n",
        "                                        encoder_optimizer, decoder_optimizer, criterion)\n",
        "            print_loss_total += loss\n",
        "            if it % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                print(f\"{timeSince(start, it / n_iters)} ({it}/{n_iters}) Loss={print_loss_avg:.4f}\")\n",
        "            it += 1\n",
        "\n",
        "# ---- evaluate include buffer setting（method 1） ----\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "        # directly setting buffer in evaluate\n",
        "        decoder.set_encoder_outputs(encoder_outputs)\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden)   # unpack为3\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words\n",
        "\n",
        "# ---- Compatible with original evaluateRandomly ----\n",
        "def evaluateRandomly_with_attn(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "# ---- Compatible with original inference ----\n",
        "def inference_with_attn(encoder, decoder, testing_pairs):\n",
        "    input_l, gt_l, predict_l = [], [], []\n",
        "    from tqdm import tqdm\n",
        "    for pair in tqdm(testing_pairs):\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        input_l.append(pair[0])\n",
        "        gt_l.append(pair[1])\n",
        "        predict_l.append(output_sentence)\n",
        "    return input_l, gt_l, predict_l\n",
        "\n"
      ],
      "metadata": {
        "id": "UXrsdvfxdHoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder_attn = EncoderRNN_Attn(input_lang.n_words, hidden_size).to(device)\n",
        "decoder_attn = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "trainIters_with_attention(encoder_attn, decoder_attn, epochs=5, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Stth402IcyJ1",
        "outputId": "ad16daf7-612d-41e3-95d0-bc3827f44b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1/5\n",
            "1m 47s (- 36m 4s) (5000/105940) Loss=3.2955\n",
            "3m 35s (- 34m 24s) (10000/105940) Loss=2.7590\n",
            "5m 24s (- 32m 45s) (15000/105940) Loss=2.5110\n",
            "7m 13s (- 31m 0s) (20000/105940) Loss=2.3154\n",
            "\n",
            "Epoch: 2/5\n",
            "9m 3s (- 29m 18s) (25000/105940) Loss=2.1506\n",
            "10m 52s (- 27m 31s) (30000/105940) Loss=1.9978\n",
            "12m 42s (- 25m 45s) (35000/105940) Loss=1.8849\n",
            "14m 32s (- 23m 58s) (40000/105940) Loss=1.7942\n",
            "\n",
            "Epoch: 3/5\n",
            "16m 22s (- 22m 10s) (45000/105940) Loss=1.6942\n",
            "18m 13s (- 20m 23s) (50000/105940) Loss=1.5920\n",
            "20m 5s (- 18m 36s) (55000/105940) Loss=1.5239\n",
            "21m 57s (- 16m 48s) (60000/105940) Loss=1.4559\n",
            "\n",
            "Epoch: 4/5\n",
            "23m 50s (- 15m 0s) (65000/105940) Loss=1.4236\n",
            "25m 42s (- 13m 12s) (70000/105940) Loss=1.3360\n",
            "27m 35s (- 11m 22s) (75000/105940) Loss=1.2721\n",
            "29m 27s (- 9m 33s) (80000/105940) Loss=1.1953\n",
            "\n",
            "Epoch: 5/5\n",
            "31m 20s (- 7m 43s) (85000/105940) Loss=1.2236\n",
            "33m 12s (- 5m 52s) (90000/105940) Loss=1.1624\n",
            "35m 4s (- 4m 2s) (95000/105940) Loss=1.1040\n",
            "36m 56s (- 2m 11s) (100000/105940) Loss=1.0687\n",
            "38m 49s (- 0m 20s) (105000/105940) Loss=1.0587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate Randomly on Training Pairs ===\")\n",
        "evaluateRandomly_with_attn(encoder_attn, decoder_attn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnBKuC-lcz38",
        "outputId": "2171f11a-30db-4dad-9770-454c09f85e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate Randomly on Training Pairs ===\n",
            "> je suis desolee mais j ai deja une petite copine .\n",
            "= i m sorry but i already have a girlfriend .\n",
            "< i m sorry but i already have a girlfriend . <EOS>\n",
            "\n",
            "> nous ne sommes pas toutes aussi fatiguees que vous .\n",
            "= we re not all as tired as you are .\n",
            "< we re not all as tired as you you you are . <EOS>\n",
            "\n",
            "> je ne vous voulais aucun mal .\n",
            "= i meant you no harm .\n",
            "< i meant you to hurt . <EOS>\n",
            "\n",
            "> je plaisante .\n",
            "= i m just joking .\n",
            "< i m not kidding . <EOS>\n",
            "\n",
            "> je suis desole nous avons ete devalises .\n",
            "= i m sorry we re completely sold out .\n",
            "< i m sorry i we ve been been . <EOS>\n",
            "\n",
            "> je suis timide .\n",
            "= i m shy .\n",
            "< i m shy . <EOS>\n",
            "\n",
            "> c est une connaissance de ma femme .\n",
            "= he is acquainted with my wife .\n",
            "< he is my wife . <EOS>\n",
            "\n",
            "> je suis un etudiant de cette ecole .\n",
            "= i am a student of this school .\n",
            "< i am a student of this school . <EOS>\n",
            "\n",
            "> j ai un peu sommeil .\n",
            "= i m feeling kind of sleepy .\n",
            "< i m feeling sleepy sleepy . <EOS>\n",
            "\n",
            "> je ne suis pas malheureux .\n",
            "= i m not miserable .\n",
            "< i m not miserable . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate on Test Set (Rouge) ===\")\n",
        "input, gt, predict = inference_with_attn(encoder_attn, decoder_attn, test_pairs)\n",
        "eval(gt, predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znyDK52JcyOt",
        "outputId": "636d390e-42ae-4db8-e408-de61232cbebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate on Test Set (Rouge) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2355/2355 [00:17<00:00, 133.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.6334235072135925\n",
            "Rouge1 precision:\t 0.5939782857894897\n",
            "Rouge1 recall:  \t 0.6873691082000732\n",
            "Rouge2 fmeasure:\t 0.4555526673793793\n",
            "Rouge2 precision:\t 0.42015090584754944\n",
            "Rouge2 recall:  \t 0.5062500834465027\n",
            "=====================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 5: Transformer Encoder + GRU Decoder"
      ],
      "metadata": {
        "id": "ObhG0lBWQChT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math  # positional encoding\n",
        "\n",
        "# -------------------------- PositionalEncoding--------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model, dropout_p, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout_p)\n",
        "        pe = torch.zeros(max_len, dim_model, device=device)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0) / dim_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pos_encoding', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x：[batch_size, seq_len, dim_model] 或 [seq_len, batch_size, dim_model]\n",
        "        seq_len = x.size(1) if x.dim() == 3 else x.size(0)\n",
        "        x = x + self.pos_encoding[:, :seq_len, :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, nhead=2, num_layers=2, dropout_p=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size  # same as Decoder hidden\n",
        "        self.dim_model = hidden_size    # Transformer input/output dimensions (aligned with hidden layer dimensions)\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, self.dim_model, device=device)\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            dim_model=self.dim_model,\n",
        "            dropout_p=dropout_p,\n",
        "            max_len=MAX_LENGTH\n",
        "        )\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=self.dim_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=self.dim_model * 4,\n",
        "            dropout=dropout_p,\n",
        "            batch_first=True,  # [batch_size, seq_len, dim_model]\n",
        "            activation='relu'\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(self.dim_model, self.hidden_size, device=device)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        # Adapts to one-dimensional input (during evaluation) and two-dimensional input (during training).\n",
        "        if input.dim() == 1:\n",
        "            # one-dimensional：[seq_len] → [1, seq_len]（batch_size=1）\n",
        "            input_seq = input.unsqueeze(0)\n",
        "        else:\n",
        "            # two-dimensional：[seq_len, 1] → [1, seq_len]（batch_size=1）\n",
        "            input_seq = input.squeeze(1).unsqueeze(0)\n",
        "\n",
        "        # input：[seq_len, 1]（original GRU input form）→ [1, seq_len]（batch_first=True）\n",
        "        # To meet the task requirements: input the entire sentence at once, rather than feeding it in word by word.\n",
        "        # input_seq = input.squeeze(1).unsqueeze(0)  # [batch_size=1, seq_len]\n",
        "\n",
        "\n",
        "        embedded = self.embedding(input_seq) * math.sqrt(torch.tensor(self.dim_model, device=device))\n",
        "\n",
        "        embedded = self.pos_encoder(embedded)  # [1, seq_len, dim_model]\n",
        "\n",
        "\n",
        "        transformer_out = self.transformer_encoder(embedded)  # [1, seq_len, dim_model]\n",
        "\n",
        "        # Generate sentence representations (core task requirement: take the average of all hidden representations of tokens).\n",
        "        sentence_embedding = transformer_out.mean(dim=1)  # [1, dim_model](Average value based on seq_len dimension)\n",
        "        sentence_embedding = self.fc(sentence_embedding).unsqueeze(0)  # [1, 1, hidden_size](Match the Decoder input shape)\n",
        "\n",
        "        encoder_outputs = self.fc(transformer_out.squeeze(0))  # [seq_len, hidden_size]\n",
        "\n",
        "        encoder_outputs_padded = torch.zeros(MAX_LENGTH, self.hidden_size, device=device)\n",
        "        encoder_outputs_padded[:encoder_outputs.size(0), :] = encoder_outputs\n",
        "\n",
        "        return encoder_outputs_padded, sentence_embedding\n",
        "\n",
        "    def initHidden(self):\n",
        "        # To maintain consistency with the original interface, the actual Transformer does not require an initial hidden state.\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderGRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size, device=device)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, device=device)\n",
        "        self.out = nn.Linear(hidden_size, output_size, device=device)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# -------------------------- train（match Transformer）--------------------------\n",
        "def train_transformer(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    loss = 0\n",
        "\n",
        "    # Transformer encodes the entire sequence at once (replacing the original GRU loop).\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / target_length\n",
        "\n",
        "def trainIters_transformer(encoder, decoder, epochs, print_every=1000, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0\n",
        "    iter = 1\n",
        "    n_iters = len(train_pairs) * epochs\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch: {epoch+1}/{epochs}\")\n",
        "        for training_pair in train_pairs:\n",
        "            training_pair = tensorsFromPair(training_pair)\n",
        "            input_tensor = training_pair[0]\n",
        "            target_tensor = training_pair[1]\n",
        "\n",
        "            loss = train_transformer(input_tensor, target_tensor, encoder,\n",
        "                                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "            print_loss_total += loss\n",
        "\n",
        "            if iter % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                print(f'{timeSince(start, iter / n_iters)} ({iter} {iter/n_iters*100:.0f}%) Loss: {print_loss_avg:.4f}')\n",
        "\n",
        "            iter += 1"
      ],
      "metadata": {
        "id": "yHmXsocr6pl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------- Model initialization and training evaluation --------------------------\n",
        "# Model parameters (consistent with the original GRU model to ensure fairness)\n",
        "hidden_size = 256\n",
        "encoder_transformer = TransformerEncoder(\n",
        "    input_size=input_lang.n_words,\n",
        "    hidden_size=hidden_size,\n",
        "    nhead=2,  # Number of attention heads (controlling model complexity)\n",
        "    num_layers=2  # The number of Transformer layers is comparable to the complexity of a single-layer GRU.\n",
        ").to(device)\n",
        "decoder_gru = DecoderGRU(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters_transformer(encoder_transformer, decoder_gru, epochs=5, print_every=5000, learning_rate=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHSn5eGb64_i",
        "outputId": "c07ad98a-fa95-42f7-96c6-566fb74ebf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/5\n",
            "1m 11s (- 23m 59s) (5000 5%) Loss: 3.5460\n",
            "2m 20s (- 22m 29s) (10000 9%) Loss: 3.1716\n",
            "3m 29s (- 21m 11s) (15000 14%) Loss: 3.0990\n",
            "4m 39s (- 20m 3s) (20000 19%) Loss: 3.0059\n",
            "\n",
            "Epoch: 2/5\n",
            "5m 50s (- 18m 54s) (25000 24%) Loss: 2.9044\n",
            "7m 0s (- 17m 45s) (30000 28%) Loss: 2.8013\n",
            "8m 11s (- 16m 36s) (35000 33%) Loss: 2.7509\n",
            "9m 22s (- 15m 26s) (40000 38%) Loss: 2.6784\n",
            "\n",
            "Epoch: 3/5\n",
            "10m 32s (- 14m 16s) (45000 42%) Loss: 2.6271\n",
            "11m 42s (- 13m 6s) (50000 47%) Loss: 2.5309\n",
            "12m 53s (- 11m 56s) (55000 52%) Loss: 2.4872\n",
            "14m 3s (- 10m 45s) (60000 57%) Loss: 2.4351\n",
            "\n",
            "Epoch: 4/5\n",
            "15m 14s (- 9m 35s) (65000 61%) Loss: 2.4240\n",
            "16m 24s (- 8m 25s) (70000 66%) Loss: 2.3335\n",
            "17m 35s (- 7m 15s) (75000 71%) Loss: 2.2818\n",
            "18m 45s (- 6m 5s) (80000 76%) Loss: 2.2528\n",
            "\n",
            "Epoch: 5/5\n",
            "19m 56s (- 4m 54s) (85000 80%) Loss: 2.2542\n",
            "21m 6s (- 3m 44s) (90000 85%) Loss: 2.1549\n",
            "22m 16s (- 2m 33s) (95000 90%) Loss: 2.1156\n",
            "23m 27s (- 1m 23s) (100000 94%) Loss: 2.1009\n",
            "24m 38s (- 0m 13s) (105000 99%) Loss: 2.0855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate Randomly on Training Pairs ===\")\n",
        "evaluateRandomly(encoder_transformer, decoder_gru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dn_GOi_QPFd",
        "outputId": "2d085bb5-6edc-454d-962a-b2f2b0434326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate Randomly on Training Pairs ===\n",
            "> je ne me sens pas tres bien .\n",
            "= i m not feeling so good .\n",
            "< you re famous . <EOS>\n",
            "\n",
            "> je suis accro a la nicotine .\n",
            "= i m addicted to nicotine .\n",
            "< you re early . <EOS>\n",
            "\n",
            "> tu n es plus seul .\n",
            "= you re not alone anymore .\n",
            "< you re off . <EOS>\n",
            "\n",
            "> tu es grognon .\n",
            "= you re grumpy .\n",
            "< you re up . <EOS>\n",
            "\n",
            "> je dois terminer mes devoirs .\n",
            "= i must get my homework finished .\n",
            "< you re famous . <EOS>\n",
            "\n",
            "> je suis trop fatiguee pour me disputer .\n",
            "= i m too tired to argue .\n",
            "< you re off . <EOS>\n",
            "\n",
            "> je le verrai demain .\n",
            "= i m going to see him tomorrow .\n",
            "< you re famous . <EOS>\n",
            "\n",
            "> tu n es pas en colere contre moi n est ce pas ?\n",
            "= you re not mad at me are you ?\n",
            "< you re off . <EOS>\n",
            "\n",
            "> il est force de remarquer votre erreur .\n",
            "= he s bound to notice your mistake .\n",
            "< you re a . <EOS>\n",
            "\n",
            "> je suis tout seul .\n",
            "= i m all by myself .\n",
            "< you re off . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate on Test Set (Rouge) ===\")\n",
        "input, gt, predict = inference(encoder_transformer, decoder_gru, test_pairs)\n",
        "eval(gt, predict)"
      ],
      "metadata": {
        "id": "pHMtZ38iQW3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628dc7bf-0a51-42fb-9f22-d3173f65d578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate on Test Set (Rouge) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2355/2355 [00:34<00:00, 69.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.13281027972698212\n",
            "Rouge1 precision:\t 0.15431706607341766\n",
            "Rouge1 recall:  \t 0.12352804094552994\n",
            "Rouge2 fmeasure:\t 0.053510479629039764\n",
            "Rouge2 precision:\t 0.06185421347618103\n",
            "Rouge2 recall:  \t 0.051382262259721756\n",
            "=====================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder_transformer = TransformerEncoder(\n",
        "    input_size=input_lang.n_words,\n",
        "    hidden_size=hidden_size,\n",
        "    nhead=2,\n",
        "    num_layers=2\n",
        ").to(device)\n",
        "decoder_gru = DecoderGRU(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "# epoch=20\n",
        "trainIters_transformer(encoder_transformer, decoder_gru, epochs=20, print_every=5000, learning_rate=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0Z9G-wwXquJ",
        "outputId": "76265f3f-9b54-4e33-e833-c5dfee8ffa1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1/20\n",
            "1m 5s (- 91m 59s) (5000 1%) Loss: 3.5291\n",
            "2m 14s (- 92m 48s) (10000 2%) Loss: 3.1865\n",
            "3m 25s (- 93m 6s) (15000 4%) Loss: 3.1044\n",
            "4m 35s (- 92m 38s) (20000 5%) Loss: 3.0125\n",
            "\n",
            "Epoch: 2/20\n",
            "5m 47s (- 92m 18s) (25000 6%) Loss: 2.8977\n",
            "6m 58s (- 91m 27s) (30000 7%) Loss: 2.8104\n",
            "8m 9s (- 90m 33s) (35000 8%) Loss: 2.7617\n",
            "9m 20s (- 89m 37s) (40000 9%) Loss: 2.6884\n",
            "\n",
            "Epoch: 3/20\n",
            "10m 32s (- 88m 39s) (45000 11%) Loss: 2.6341\n",
            "11m 43s (- 87m 35s) (50000 12%) Loss: 2.5447\n",
            "12m 54s (- 86m 31s) (55000 13%) Loss: 2.5135\n",
            "14m 5s (- 85m 26s) (60000 14%) Loss: 2.4461\n",
            "\n",
            "Epoch: 4/20\n",
            "15m 17s (- 84m 22s) (65000 15%) Loss: 2.4281\n",
            "16m 28s (- 83m 14s) (70000 17%) Loss: 2.3305\n",
            "17m 39s (- 82m 6s) (75000 18%) Loss: 2.3017\n",
            "18m 50s (- 80m 58s) (80000 19%) Loss: 2.2493\n",
            "\n",
            "Epoch: 5/20\n",
            "20m 2s (- 79m 50s) (85000 20%) Loss: 2.2557\n",
            "21m 13s (- 78m 42s) (90000 21%) Loss: 2.1749\n",
            "22m 24s (- 77m 31s) (95000 22%) Loss: 2.1377\n",
            "23m 35s (- 76m 22s) (100000 24%) Loss: 2.1165\n",
            "24m 46s (- 75m 13s) (105000 25%) Loss: 2.0955\n",
            "\n",
            "Epoch: 6/20\n",
            "25m 58s (- 74m 4s) (110000 26%) Loss: 2.0308\n",
            "27m 9s (- 72m 55s) (115000 27%) Loss: 1.9876\n",
            "28m 20s (- 71m 45s) (120000 28%) Loss: 1.9799\n",
            "29m 32s (- 70m 36s) (125000 29%) Loss: 1.9553\n",
            "\n",
            "Epoch: 7/20\n",
            "30m 43s (- 69m 26s) (130000 31%) Loss: 1.9005\n",
            "31m 54s (- 68m 16s) (135000 32%) Loss: 1.8689\n",
            "33m 6s (- 67m 6s) (140000 33%) Loss: 1.8393\n",
            "34m 17s (- 65m 55s) (145000 34%) Loss: 1.8144\n",
            "\n",
            "Epoch: 8/20\n",
            "35m 29s (- 64m 45s) (150000 35%) Loss: 1.7907\n",
            "36m 40s (- 63m 35s) (155000 37%) Loss: 1.7361\n",
            "37m 52s (- 62m 25s) (160000 38%) Loss: 1.7160\n",
            "39m 3s (- 61m 15s) (165000 39%) Loss: 1.6820\n",
            "\n",
            "Epoch: 9/20\n",
            "40m 15s (- 60m 5s) (170000 40%) Loss: 1.6964\n",
            "41m 26s (- 58m 54s) (175000 41%) Loss: 1.6338\n",
            "42m 38s (- 57m 44s) (180000 42%) Loss: 1.6106\n",
            "43m 48s (- 56m 32s) (185000 44%) Loss: 1.5819\n",
            "45m 0s (- 55m 21s) (190000 45%) Loss: 1.5934\n",
            "\n",
            "Epoch: 10/20\n",
            "46m 11s (- 54m 11s) (195000 46%) Loss: 1.5322\n",
            "47m 22s (- 53m 0s) (200000 47%) Loss: 1.5107\n",
            "48m 34s (- 51m 50s) (205000 48%) Loss: 1.4935\n",
            "49m 46s (- 50m 39s) (210000 50%) Loss: 1.4906\n",
            "\n",
            "Epoch: 11/20\n",
            "50m 57s (- 49m 28s) (215000 51%) Loss: 1.4360\n",
            "52m 9s (- 48m 18s) (220000 52%) Loss: 1.4128\n",
            "53m 20s (- 47m 7s) (225000 53%) Loss: 1.4093\n",
            "54m 32s (- 45m 56s) (230000 54%) Loss: 1.4018\n",
            "\n",
            "Epoch: 12/20\n",
            "55m 43s (- 44m 45s) (235000 55%) Loss: 1.3639\n",
            "56m 55s (- 43m 35s) (240000 57%) Loss: 1.3358\n",
            "58m 7s (- 42m 24s) (245000 58%) Loss: 1.3099\n",
            "59m 18s (- 41m 13s) (250000 59%) Loss: 1.2944\n",
            "\n",
            "Epoch: 13/20\n",
            "60m 30s (- 40m 2s) (255000 60%) Loss: 1.3203\n",
            "61m 41s (- 38m 51s) (260000 61%) Loss: 1.2582\n",
            "62m 54s (- 37m 40s) (265000 63%) Loss: 1.2442\n",
            "64m 5s (- 36m 30s) (270000 64%) Loss: 1.2223\n",
            "65m 17s (- 35m 19s) (275000 65%) Loss: 1.2480\n",
            "\n",
            "Epoch: 14/20\n",
            "66m 30s (- 34m 8s) (280000 66%) Loss: 1.1772\n",
            "67m 41s (- 32m 57s) (285000 67%) Loss: 1.1714\n",
            "68m 52s (- 31m 46s) (290000 68%) Loss: 1.1502\n",
            "70m 3s (- 30m 34s) (295000 70%) Loss: 1.1590\n",
            "\n",
            "Epoch: 15/20\n",
            "71m 15s (- 29m 23s) (300000 71%) Loss: 1.1199\n",
            "72m 26s (- 28m 12s) (305000 72%) Loss: 1.1138\n",
            "73m 37s (- 27m 1s) (310000 73%) Loss: 1.1012\n",
            "74m 49s (- 25m 50s) (315000 74%) Loss: 1.0944\n",
            "\n",
            "Epoch: 16/20\n",
            "76m 0s (- 24m 38s) (320000 76%) Loss: 1.0904\n",
            "77m 12s (- 23m 27s) (325000 77%) Loss: 1.0504\n",
            "78m 23s (- 22m 16s) (330000 78%) Loss: 1.0263\n",
            "79m 34s (- 21m 5s) (335000 79%) Loss: 1.0250\n",
            "\n",
            "Epoch: 17/20\n",
            "80m 46s (- 19m 53s) (340000 80%) Loss: 1.0411\n",
            "81m 57s (- 18m 42s) (345000 81%) Loss: 0.9890\n",
            "83m 9s (- 17m 31s) (350000 83%) Loss: 0.9736\n",
            "84m 20s (- 16m 20s) (355000 84%) Loss: 0.9609\n",
            "85m 32s (- 15m 9s) (360000 85%) Loss: 0.9940\n",
            "\n",
            "Epoch: 18/20\n",
            "86m 44s (- 13m 57s) (365000 86%) Loss: 0.9661\n",
            "87m 55s (- 12m 46s) (370000 87%) Loss: 0.9348\n",
            "89m 7s (- 11m 35s) (375000 88%) Loss: 0.9214\n",
            "90m 18s (- 10m 24s) (380000 90%) Loss: 0.9477\n",
            "\n",
            "Epoch: 19/20\n",
            "91m 30s (- 9m 12s) (385000 91%) Loss: 0.9100\n",
            "92m 41s (- 8m 1s) (390000 92%) Loss: 0.8935\n",
            "93m 53s (- 6m 50s) (395000 93%) Loss: 0.8802\n",
            "95m 5s (- 5m 38s) (400000 94%) Loss: 0.8957\n",
            "\n",
            "Epoch: 20/20\n",
            "96m 18s (- 4m 27s) (405000 96%) Loss: 0.8775\n",
            "97m 30s (- 3m 16s) (410000 97%) Loss: 0.8571\n",
            "98m 42s (- 2m 5s) (415000 98%) Loss: 0.8411\n",
            "99m 56s (- 0m 53s) (420000 99%) Loss: 0.8390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate Randomly on Training Pairs ===\")\n",
        "evaluateRandomly(encoder_transformer, decoder_gru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlVgh_X8XziS",
        "outputId": "c6c20b31-ba1d-4295-c00c-27134762927d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate Randomly on Training Pairs ===\n",
            "> il commande rarement quoi que ce soit de nouveau .\n",
            "= he seldom orders anything new .\n",
            "< they . <EOS>\n",
            "\n",
            "> je suis maintenant habituee au port du masque .\n",
            "= i m now used to wearing a mask .\n",
            "< they . <EOS>\n",
            "\n",
            "> nous envisageons l achat de nouveaux meubles .\n",
            "= we re thinking of buying some new furniture .\n",
            "< they re out . <EOS>\n",
            "\n",
            "> je quitte l australie .\n",
            "= i m leaving australia .\n",
            "< they . <EOS>\n",
            "\n",
            "> il n est plus ce qu il etait .\n",
            "= he is not what he was .\n",
            "< <EOS>\n",
            "\n",
            "> j ai du mal a me concentrer .\n",
            "= i m having trouble focusing .\n",
            "< they re up . <EOS>\n",
            "\n",
            "> tu n es pas non plus un saint .\n",
            "= you re not a saint either .\n",
            "< they . <EOS>\n",
            "\n",
            "> il est tres peu probable que je sois licencie pour avoir fait ca .\n",
            "= i m not very likely to be fired for doing that .\n",
            "< <EOS>\n",
            "\n",
            "> je suis toujours votre ami .\n",
            "= i m still your friend .\n",
            "< they . <EOS>\n",
            "\n",
            "> vous etes les etudiantes de tom n est ce pas ?\n",
            "= you re tom s students right ?\n",
            "< they . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Evaluate on Test Set (Rouge) ===\")\n",
        "input, gt, predict = inference(encoder_transformer, decoder_gru, test_pairs)\n",
        "eval(gt, predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M1bB3iwX2KF",
        "outputId": "ec0c15fc-647a-467d-820b-0530366b76a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluate on Test Set (Rouge) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2355/2355 [00:32<00:00, 71.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.02623540721833706\n",
            "Rouge1 precision:\t 0.03557678684592247\n",
            "Rouge1 recall:  \t 0.022507069632411003\n",
            "Rouge2 fmeasure:\t 0.00218442571349442\n",
            "Rouge2 precision:\t 0.0026185421738773584\n",
            "Rouge2 recall:  \t 0.0021834815852344036\n",
            "=====================================\n"
          ]
        }
      ]
    }
  ]
}